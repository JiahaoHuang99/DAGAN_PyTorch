{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%import\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from model import *\n",
    "from utils import *\n",
    "from config import config, log_config\n",
    "from scipy.io import loadmat, savemat\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def main_train(model_name, mask_name, mask_perc):\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    print('[*] run basic configs ... ')\n",
    "\n",
    "    # 生成log地址，创建log文件夹\n",
    "    log_dir = \"log_{}_{}_{}\".format(model_name, mask_name, mask_perc)\n",
    "    tl.files.exists_or_mkdir(log_dir)\n",
    "    log_all, log_eval, log_50, log_all_filename, log_eval_filename, log_50_filename = logging_setup(log_dir)\n",
    "\n",
    "    checkpoint_dir = \"checkpoint_{}_{}_{}\".format(model_name, mask_name, mask_perc)\n",
    "    tl.files.exists_or_mkdir(checkpoint_dir)\n",
    "\n",
    "    save_dir = \"samples_{}_{}_{}\".format(model_name, mask_name, mask_perc)\n",
    "    tl.files.exists_or_mkdir(save_dir)\n",
    "\n",
    "    # 读取参数\n",
    "    batch_size = config.TRAIN.batch_size\n",
    "    early_stopping_num = config.TRAIN.early_stopping_num\n",
    "    g_alpha = config.TRAIN.g_alpha\n",
    "    g_beta = config.TRAIN.g_beta\n",
    "    g_gamma = config.TRAIN.g_gamma\n",
    "    g_adv = config.TRAIN.g_adv\n",
    "    lr = config.TRAIN.lr\n",
    "    lr_decay = config.TRAIN.lr_decay\n",
    "    decay_every = config.TRAIN.decay_every\n",
    "    beta1 = config.TRAIN.beta1\n",
    "    n_epoch = config.TRAIN.n_epoch\n",
    "    sample_size = config.TRAIN.sample_size\n",
    "\n",
    "    log_config(log_all_filename, config)\n",
    "    log_config(log_eval_filename, config)\n",
    "    log_config(log_50_filename, config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% # =================================== BASIC CONFIGS =================================== #\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    print('[*] load data ... ')\n",
    "\n",
    "    # 数据集路径\n",
    "    training_data_path = config.TRAIN.training_data_path\n",
    "    val_data_path = config.TRAIN.val_data_path\n",
    "    testing_data_path = config.TRAIN.testing_data_path\n",
    "\n",
    "    # 读取数据\n",
    "    with open(training_data_path, 'rb') as f:\n",
    "        X_train = pickle.load(f)\n",
    "\n",
    "    with open(val_data_path, 'rb') as f:\n",
    "        X_val = pickle.load(f)\n",
    "\n",
    "    with open(testing_data_path, 'rb') as f:\n",
    "        X_test = pickle.load(f)\n",
    "\n",
    "    print('X_train shape/min/max: ', X_train.shape, X_train.min(), X_train.max())\n",
    "    print('X_val shape/min/max: ', X_val.shape, X_val.min(), X_val.max())\n",
    "    print('X_test shape/min/max: ', X_test.shape, X_test.min(), X_test.max())\n",
    "\n",
    "    # 读取mask\n",
    "    print('[*] loading mask ... ')\n",
    "    if mask_name == \"gaussian2d\":\n",
    "        mask = \\\n",
    "            loadmat(\n",
    "                os.path.join(config.TRAIN.mask_Gaussian2D_path, \"GaussianDistribution2DMask_{}.mat\".format(mask_perc)))[\n",
    "                'maskRS2']\n",
    "    elif mask_name == \"gaussian1d\":\n",
    "        mask = \\\n",
    "            loadmat(\n",
    "                os.path.join(config.TRAIN.mask_Gaussian1D_path, \"GaussianDistribution1DMask_{}.mat\".format(mask_perc)))[\n",
    "                'maskRS1']\n",
    "    elif mask_name == \"poisson2d\":\n",
    "        mask = \\\n",
    "            loadmat(\n",
    "                os.path.join(config.TRAIN.mask_Gaussian1D_path, \"PoissonDistributionMask_{}.mat\".format(mask_perc)))[\n",
    "                'population_matrix']\n",
    "    else:\n",
    "        raise ValueError(\"no such mask exists: {}\".format(mask_name))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% # ==================================== PREPARE DATA ==================================== #\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    # 模型定义\n",
    "    print('[*] define model ... ')\n",
    "    # 读取图片大小\n",
    "    nw, nh, nz = X_train.shape[1:]\n",
    "\n",
    "    # define placeholders??? （改）\n",
    "    t_image_good = tf.placeholder('float32', [batch_size, nw, nh, nz], name='good_image')\n",
    "    t_image_good_samples = tf.placeholder('float32', [sample_size, nw, nh, nz], name='good_image_samples')\n",
    "    t_image_bad = tf.placeholder('float32', [batch_size, nw, nh, nz], name='bad_image')\n",
    "    t_image_bad_samples = tf.placeholder('float32', [sample_size, nw, nh, nz], name='bad_image_samples')\n",
    "    t_gen = tf.placeholder('float32', [batch_size, nw, nh, nz], name='generated_image_for_test')\n",
    "    t_gen_sample = tf.placeholder('float32', [sample_size, nw, nh, nz], name='generated_sample_image_for_test')\n",
    "    t_image_good_244 = tf.placeholder('float32', [batch_size, 244, 244, 3], name='vgg_good_image')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% # ==================================== DEFINE MODEL ==================================== #\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    unet = UNet()\n",
    "    if tl.global_flag['model'] == 'unet':\n",
    "        net = unet.forward(t_image_bad, is_train=True, reuse=False, is_refine=False)\n",
    "        net_test = unet.forward(t_image_bad, is_train=False, reuse=True, is_refine=False)\n",
    "        net_test_sample = unet.forward(t_image_bad_samples, is_train=False, reuse=True, is_refine=False)\n",
    "    elif tl.global_flag['model'] == 'unet_refine':\n",
    "        net = unet.forward(t_image_bad, is_train=True, reuse=False, is_refine=True)\n",
    "        net_test = unet.forward(t_image_bad, is_train=False, reuse=True, is_refine=True)\n",
    "        net_test_sample = unet.forward(t_image_bad_samples, is_train=False, reuse=True, is_refine=True)\n",
    "    else:\n",
    "        raise Exception(\"unknown model\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% # 定义生成器网络\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    discriminator = Discriminator()\n",
    "    net_d, logits_fake = discriminator.forward(net.outputs, is_train=True, reuse=False)\n",
    "    _, logits_real = discriminator.forward(t_image_good, is_train=True, reuse=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% # 定义鉴别器网络\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    vgg = VGG()\n",
    "    net_vgg_conv4_good, _ = vgg.forward(t_image_good_244, reuse=False)\n",
    "    net_vgg_conv4_gen, _ = vgg.forward(tf.tile(tf.image.resize_images(net.outputs, [244, 244]), [1, 1, 1, 3]), reuse=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% # 定义VGG网络\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    print('[*] define loss functions ... ')\n",
    "\n",
    "    sigmoid_cross_entropy = nn.CrossEntropyLoss()\n",
    "    # discriminator loss\n",
    "    d_loss1 = sigmoid_cross_entropy(logits_real, torch.ones_like(logits_real))\n",
    "    d_loss2 = sigmoid_cross_entropy(logits_fake, torch.zeros_like(logits_fake))\n",
    "    d_loss = d_loss1 + d_loss2\n",
    "\n",
    "    # generator loss (adversarial)\n",
    "    g_loss = sigmoid_cross_entropy(logits_fake, tf.ones_like(logits_fake))\n",
    "\n",
    "    # generator loss (perceptual)\n",
    "    g_perceptual = torch.mean(torch.mean(torch.sub(net_vgg_conv4_good.outputs,net_vgg_conv4_gen.outputs).pow(2), dim=[1, 2, 3]))\n",
    "\n",
    "    # generator loss (pixel-wise)\n",
    "    g_nmse_a = torch.sqrt(torch.sum(torch.sub(net.outputs, t_image_good).pow(2), dim=[1, 2, 3]))\n",
    "    g_nmse_b = torch.sqrt(torch.sum(t_image_good.pow(2), dim=[1, 2, 3]))\n",
    "    g_nmse = torch.mean(g_nmse_a / g_nmse_b)\n",
    "\n",
    "    # generator loss (frequency)\n",
    "    fft_good_abs = torch.Tensor.map_(t_image_good,fft_abs_for_map_fn)\n",
    "    fft_gen_abs = torch.Tensor.map_(net.outputs,fft_abs_for_map_fn)\n",
    "    g_fft = torch.mean(torch.mean(torch.sub(fft_good_abs, fft_gen_abs).pow(2), dim=[1, 2]))\n",
    "\n",
    "    # generator loss (total)\n",
    "    g_loss = g_adv * g_loss + g_alpha * g_nmse + g_gamma * g_perceptual + g_beta * g_fft\n",
    "\n",
    "    # nmse metric for testing purpose\n",
    "    nmse_a_0_1 = torch.sqrt(torch.sum(torch.sub(t_gen, t_image_good).pow(2), dim=[1, 2, 3]))\n",
    "    nmse_b_0_1 = torch.sqrt(torch.sum(t_image_good.pow(2), dim=[1, 2, 3]))\n",
    "    nmse_0_1 = nmse_a_0_1 / nmse_b_0_1\n",
    "\n",
    "    nmse_a_0_1_sample = torch.sqrt(torch.sum(torch.sub(t_gen_sample, t_image_good_samples).pow(2), dim=[1, 2, 3]))\n",
    "    nmse_b_0_1_sample = torch.sqrt(torch.sum(t_image_good_samples.pow(2), dim=[1, 2, 3]))\n",
    "    nmse_0_1_sample = nmse_a_0_1_sample / nmse_b_0_1_sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% # ==================================== DEFINE LOSS ==================================== #\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    print('[*] define training options ... ')\n",
    "\n",
    "    g_vars = tl.layers.get_variables_with_name('u_net', True, True)\n",
    "    d_vars = tl.layers.get_variables_with_name('discriminator', True, True)\n",
    "\n",
    "    with tf.variable_scope('learning_rate'):\n",
    "        lr_v = tf.Variable(lr, trainable=False)\n",
    "\n",
    "    g_optim = tf.train.AdamOptimizer(lr_v, beta1=beta1).minimize(g_loss, var_list=g_vars)\n",
    "    d_optim = tf.train.AdamOptimizer(lr_v, beta1=beta1).minimize(d_loss, var_list=d_vars)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%??? # ==================================== DEFINE TRAIN OPTS ==================================== #\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "    tl.layers.initialize_global_variables(sess)\n",
    "\n",
    "    # 加载生成器和鉴别器的权重文件 load generator and discriminator weights (for continuous training purpose)\n",
    "    tl.files.load_and_assign_npz(sess=sess,\n",
    "                                 name=os.path.join(checkpoint_dir, tl.global_flag['model']) + '.npz',\n",
    "                                 network=net)\n",
    "    tl.files.load_and_assign_npz(sess=sess,\n",
    "                                 name=os.path.join(checkpoint_dir, tl.global_flag['model']) + '_d.npz',\n",
    "                                 network=net_d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%??? # ==================================== TRAINING ==================================== #\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    # vgg模型地址\n",
    "    net_vgg_conv4_path = config.TRAIN.VGG16_path\n",
    "\n",
    "    npz = np.load(net_vgg_conv4_path)\n",
    "    assign_op = []\n",
    "    for idx, val in enumerate(sorted(npz.items())[0:20]):\n",
    "        print(\"  Loading pretrained VGG16, CNN part %s\" % str(val[1].shape))\n",
    "        assign_op.append(net_vgg_conv4_good.all_params[idx].assign(val[1]))\n",
    "    sess.run(assign_op)\n",
    "    net_vgg_conv4_good.print_params(False)\n",
    "\n",
    "    n_training_examples = len(X_train)\n",
    "    n_step_epoch = round(n_training_examples / batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% # load vgg weights ???????\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    # 随机生成sample_size个idex\n",
    "    idex = tl.utils.get_random_int(min=0, max=len(X_test) - 1, number=sample_size, seed=config.TRAIN.seed)\n",
    "    # 好图sample\n",
    "    X_samples_good = X_test[idex]\n",
    "    #  处理图片变成坏图sample\n",
    "    X_samples_bad = threading_data(X_samples_good, fn=to_bad_img, mask=mask)\n",
    "    # ???\n",
    "    x_good_sample_rescaled = (X_samples_good + 1) / 2\n",
    "    x_bad_sample_rescaled = (X_samples_bad + 1) / 2\n",
    "    # 存sample图\n",
    "    tl.visualize.save_images(X_samples_good,\n",
    "                             [5, 10],\n",
    "                             os.path.join(save_dir, \"sample_image_good.png\"))\n",
    "\n",
    "    tl.visualize.save_images(X_samples_bad,\n",
    "                             [5, 10],\n",
    "                             os.path.join(save_dir, \"sample_image_bad.png\"))\n",
    "\n",
    "    tl.visualize.save_images(np.abs(X_samples_good - X_samples_bad),\n",
    "                             [5, 10],\n",
    "                             os.path.join(save_dir, \"sample_image_diff_abs.png\"))\n",
    "\n",
    "    tl.visualize.save_images(np.sqrt(np.abs(X_samples_good - X_samples_bad) / 2 + config.TRAIN.epsilon),\n",
    "                             [5, 10],\n",
    "                             os.path.join(save_dir, \"sample_image_diff_sqrt_abs.png\"))\n",
    "\n",
    "    tl.visualize.save_images(np.clip(10 * np.abs(X_samples_good - X_samples_bad) / 2, 0, 1),\n",
    "                             [5, 10],\n",
    "                             os.path.join(save_dir, \"sample_image_diff_sqrt_abs_10_clip.png\"))\n",
    "\n",
    "    tl.visualize.save_images(threading_data(X_samples_good, fn=distort_img),\n",
    "                             [5, 10],\n",
    "                             os.path.join(save_dir, \"sample_image_aug.png\"))\n",
    "    scipy.misc.imsave(os.path.join(save_dir, \"mask.png\"), mask * 255)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% # sample testing images\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    best_nmse = np.inf  # 初始化\n",
    "    best_epoch = 1  # 初始化\n",
    "    esn = early_stopping_num  # 早停\n",
    "    for epoch in range(0, n_epoch):"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%     print('[*] start training ... ')\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "        # learning rate decay\n",
    "        if epoch != 0 and (epoch % decay_every == 0):\n",
    "            new_lr_decay = lr_decay ** (epoch // decay_every)\n",
    "            sess.run(tf.assign(lr_v, lr * new_lr_decay))\n",
    "            log = \" ** new learning rate: %f\" % (lr * new_lr_decay)\n",
    "            print(log)\n",
    "            log_all.debug(log)\n",
    "        elif epoch == 0:\n",
    "            log = \" ** init lr: %f  decay_every_epoch: %d, lr_decay: %f\" % (lr, decay_every, lr_decay)\n",
    "            print(log)\n",
    "            log_all.debug(log)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%  learning rate\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "        for step in range(n_step_epoch):\n",
    "            # time\n",
    "            step_time = time.time()\n",
    "            # 生成batch_size个随机初始idex\n",
    "            idex = tl.utils.get_random_int(min=0, max=n_training_examples - 1, number=batch_size)\n",
    "            # real picture (good)\n",
    "            X_good = X_train[idex]\n",
    "            # 好图变坏图\n",
    "            X_good_aug = torch.Tensor.map_(X_good, distort_img)\n",
    "            # 更改格式用于输入VGG\n",
    "            X_good_244 = torch.Tensor.map_(X_good_aug, vgg_prepro)\n",
    "            X_bad = torch.Tensor.map_(X_good_aug, mask, to_bad_img)  # ??? X_good\n",
    "            # 判别器误差\n",
    "            errD, _ = sess.run([d_loss, d_optim], {t_image_good: X_good_aug, t_image_bad: X_bad})\n",
    "            # 生成器误差， 生成器感知误差，生成器像素均方差，生成器频域均方差\n",
    "            errG, errG_perceptual, errG_nmse, errG_fft, _ = sess.run([g_loss, g_perceptual, g_nmse, g_fft, g_optim],\n",
    "                                                                     {t_image_good_244: X_good_244,\n",
    "                                                                      t_image_good: X_good_aug,\n",
    "                                                                      t_image_bad: X_bad})\n",
    "            # log名\n",
    "            log = \"Epoch[{:3}/{:3}] step={:3} d_loss={:5} g_loss={:5} g_perceptual_loss={:5} g_mse={:5} g_freq={:5} took {:3}s\".format(\n",
    "                epoch + 1,\n",
    "                n_epoch,\n",
    "                step,\n",
    "                round(float(errD), 3),\n",
    "                round(float(errG), 3),\n",
    "                round(float(errG_perceptual), 3),\n",
    "                round(float(errG_nmse), 3),\n",
    "                round(float(errG_fft), 3),\n",
    "                round(time.time() - step_time, 2))\n",
    "\n",
    "            print(log)\n",
    "            log_all.debug(log)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "        # evaluation for training data\n",
    "        total_nmse_training = 0  # 初始化\n",
    "        total_ssim_training = 0  # 初始化\n",
    "        total_psnr_training = 0  # 初始化\n",
    "        num_training_temp = 0  # 初始化\n",
    "        # 生成小batch\n",
    "        for batch in tl.iterate.minibatches(inputs=X_train, targets=X_train, batch_size=batch_size, shuffle=False):\n",
    "            x_good, _ = batch\n",
    "            x_bad = threading_data(x_good, fn=to_bad_img, mask=mask)\n",
    "\n",
    "            x_gen = sess.run(net_test.outputs, {t_image_bad: x_bad})\n",
    "\n",
    "            x_good_0_1 = (x_good + 1) / 2\n",
    "            x_gen_0_1 = (x_gen + 1) / 2\n",
    "            # 每minibatch的评估参数\n",
    "            nmse_res = sess.run(nmse_0_1, {t_gen: x_gen_0_1, t_image_good: x_good_0_1})\n",
    "            ssim_res = threading_data([_ for _ in zip(x_good_0_1, x_gen_0_1)], fn=ssim)\n",
    "            psnr_res = threading_data([_ for _ in zip(x_good_0_1, x_gen_0_1)], fn=psnr)\n",
    "            # 总batch的评估参数\n",
    "            total_nmse_training += np.sum(nmse_res)\n",
    "            total_ssim_training += np.sum(ssim_res)\n",
    "            total_psnr_training += np.sum(psnr_res)\n",
    "            num_training_temp += batch_size\n",
    "        # 评估参数求平均\n",
    "        total_nmse_training /= num_training_temp\n",
    "        total_ssim_training /= num_training_temp\n",
    "        total_psnr_training /= num_training_temp\n",
    "\n",
    "        # 生成log\n",
    "        log = \"Epoch: {}\\nNMSE training: {:8}, SSIM training: {:8}, PSNR training: {:8}\".format(\n",
    "            epoch + 1,\n",
    "            total_nmse_training,\n",
    "            total_ssim_training,\n",
    "            total_psnr_training)\n",
    "        print(log)\n",
    "        log_all.debug(log)\n",
    "        log_eval.info(log)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "        # evaluation for validation data\n",
    "        total_nmse_val = 0\n",
    "        total_ssim_val = 0\n",
    "        total_psnr_val = 0\n",
    "        num_val_temp = 0\n",
    "        for batch in tl.iterate.minibatches(inputs=X_val, targets=X_val, batch_size=batch_size, shuffle=False):\n",
    "            x_good, _ = batch\n",
    "            # x_bad = threading_data(x_good, fn=to_bad_img, mask=mask)\n",
    "            x_bad = threading_data(\n",
    "                x_good,\n",
    "                fn=to_bad_img,\n",
    "                mask=mask)\n",
    "\n",
    "            x_gen = sess.run(net_test.outputs, {t_image_bad: x_bad})\n",
    "\n",
    "            x_good_0_1 = (x_good + 1) / 2\n",
    "            x_gen_0_1 = (x_gen + 1) / 2\n",
    "\n",
    "            nmse_res = sess.run(nmse_0_1, {t_gen: x_gen_0_1, t_image_good: x_good_0_1})\n",
    "            ssim_res = threading_data([_ for _ in zip(x_good_0_1, x_gen_0_1)], fn=ssim)\n",
    "            psnr_res = threading_data([_ for _ in zip(x_good_0_1, x_gen_0_1)], fn=psnr)\n",
    "            total_nmse_val += np.sum(nmse_res)\n",
    "            total_ssim_val += np.sum(ssim_res)\n",
    "            total_psnr_val += np.sum(psnr_res)\n",
    "            num_val_temp += batch_size\n",
    "\n",
    "        total_nmse_val /= num_val_temp\n",
    "        total_ssim_val /= num_val_temp\n",
    "        total_psnr_val /= num_val_temp\n",
    "\n",
    "        # 生成log\n",
    "        log = \"Epoch: {}\\nNMSE val: {:8}, SSIM val: {:8}, PSNR val: {:8}\".format(\n",
    "            epoch + 1,\n",
    "            total_nmse_val,\n",
    "            total_ssim_val,\n",
    "            total_psnr_val)\n",
    "        print(log)\n",
    "        log_all.debug(log)\n",
    "        log_eval.info(log)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "        img = sess.run(net_test_sample.outputs, {t_image_bad_samples: X_samples_bad})\n",
    "        tl.visualize.save_images(img,\n",
    "                                 [5, 10],\n",
    "                                 os.path.join(save_dir, \"image_{}.png\".format(epoch)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% 存图\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "        if total_nmse_val < best_nmse:\n",
    "            esn = early_stopping_num  # reset early stopping num\n",
    "            best_nmse = total_nmse_val\n",
    "            best_epoch = epoch + 1\n",
    "\n",
    "            # save current best model\n",
    "            tl.files.save_npz(net.all_params,\n",
    "                              name=os.path.join(checkpoint_dir, tl.global_flag['model']) + '.npz',\n",
    "                              sess=sess)\n",
    "\n",
    "            tl.files.save_npz(net_d.all_params,\n",
    "                              name=os.path.join(checkpoint_dir, tl.global_flag['model']) + '_d.npz',\n",
    "                              sess=sess)\n",
    "            print(\"[*] Save checkpoints SUCCESS!\")\n",
    "        else:\n",
    "            esn -= 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% 早停计数器\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "        log = \"Best NMSE result: {} at {} epoch\".format(best_nmse, best_epoch)\n",
    "        log_eval.info(log)\n",
    "        log_all.debug(log)\n",
    "        print(log)\n",
    "\n",
    "        # early stopping triggered\n",
    "        if esn == 0:\n",
    "            log_eval.info(log)\n",
    "\n",
    "            tl.files.load_and_assign_npz(sess=sess,\n",
    "                                         name=os.path.join(checkpoint_dir, tl.global_flag['model']) + '.npz',\n",
    "                                         network=net)\n",
    "            # evluation for test data\n",
    "            x_gen = sess.run(net_test_sample.outputs, {t_image_bad_samples: X_samples_bad})\n",
    "            x_gen_0_1 = (x_gen + 1) / 2\n",
    "            savemat(save_dir + '/test_random_50_generated.mat', {'x_gen_0_1': x_gen_0_1})\n",
    "\n",
    "            nmse_res = sess.run(nmse_0_1_sample, {t_gen_sample: x_gen_0_1, t_image_good_samples: x_good_sample_rescaled})\n",
    "            ssim_res = threading_data([_ for _ in zip(x_good_sample_rescaled, x_gen_0_1)], fn=ssim)\n",
    "            psnr_res = threading_data([_ for _ in zip(x_good_sample_rescaled, x_gen_0_1)], fn=psnr)\n",
    "\n",
    "            log = \"NMSE testing: {}\\nSSIM testing: {}\\nPSNR testing: {}\\n\\n\".format(\n",
    "                nmse_res,\n",
    "                ssim_res,\n",
    "                psnr_res)\n",
    "\n",
    "            log_50.debug(log)\n",
    "\n",
    "            log = \"NMSE testing average: {}\\nSSIM testing average: {}\\nPSNR testing average: {}\\n\\n\".format(\n",
    "                np.mean(nmse_res),\n",
    "                np.mean(ssim_res),\n",
    "                np.mean(psnr_res))\n",
    "\n",
    "            log_50.debug(log)\n",
    "\n",
    "            log = \"NMSE testing std: {}\\nSSIM testing std: {}\\nPSNR testing std: {}\\n\\n\".format(np.std(nmse_res),\n",
    "                                                                                                np.std(ssim_res),\n",
    "                                                                                                np.std(psnr_res))\n",
    "\n",
    "            log_50.debug(log)\n",
    "\n",
    "            # evaluation for zero-filled (ZF) data\n",
    "            nmse_res_zf = sess.run(nmse_0_1_sample,\n",
    "                                   {t_gen_sample: x_bad_sample_rescaled, t_image_good_samples: x_good_sample_rescaled})\n",
    "            ssim_res_zf = threading_data([_ for _ in zip(x_good_sample_rescaled, x_bad_sample_rescaled)], fn=ssim)\n",
    "            psnr_res_zf = threading_data([_ for _ in zip(x_good_sample_rescaled, x_bad_sample_rescaled)], fn=psnr)\n",
    "\n",
    "            log = \"NMSE ZF testing: {}\\nSSIM ZF testing: {}\\nPSNR ZF testing: {}\\n\\n\".format(\n",
    "                nmse_res_zf,\n",
    "                ssim_res_zf,\n",
    "                psnr_res_zf)\n",
    "\n",
    "            log_50.debug(log)\n",
    "\n",
    "            log = \"NMSE ZF average testing: {}\\nSSIM ZF average testing: {}\\nPSNR ZF average testing: {}\\n\\n\".format(\n",
    "                np.mean(nmse_res_zf),\n",
    "                np.mean(ssim_res_zf),\n",
    "                np.mean(psnr_res_zf))\n",
    "\n",
    "            log_50.debug(log)\n",
    "\n",
    "            log = \"NMSE ZF std testing: {}\\nSSIM ZF std testing: {}\\nPSNR ZF std testing: {}\\n\\n\".format(\n",
    "                np.std(nmse_res_zf),\n",
    "                np.std(ssim_res_zf),\n",
    "                np.std(psnr_res_zf))\n",
    "\n",
    "            log_50.debug(log)\n",
    "\n",
    "            # sample testing images\n",
    "            tl.visualize.save_images(x_gen,\n",
    "                                     [5, 10],\n",
    "                                     os.path.join(save_dir, \"final_generated_image.png\"))\n",
    "\n",
    "            tl.visualize.save_images(np.clip(10 * np.abs(X_samples_good - x_gen) / 2, 0, 1),\n",
    "                                     [5, 10],\n",
    "                                     os.path.join(save_dir, \"final_generated_image_diff_abs_10_clip.png\"))\n",
    "\n",
    "            tl.visualize.save_images(np.clip(10 * np.abs(X_samples_good - X_samples_bad) / 2, 0, 1),\n",
    "                                     [5, 10],\n",
    "                                     os.path.join(save_dir, \"final_bad_image_diff_abs_10_clip.png\"))\n",
    "\n",
    "            print(\"[*] Job finished!\")\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% best\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--model', type=str, default='unet', help='unet, unet_refine')\n",
    "    parser.add_argument('--mask', type=str, default='gaussian2d', help='gaussian1d, gaussian2d, poisson2d')\n",
    "    parser.add_argument('--maskperc', type=int, default='30', help='10,20,30,40,50')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main_train(args.model, args.mask, args.maskperc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}